"""
AI Service for MBTI personality analysis using Google Gemini.
Implements hybrid model approach, structured output, adaptive questioning, and multi-depth analysis modes.
"""
import json
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Optional

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from pydantic import BaseModel, Field, ValidationError

from app.config import settings

logger = logging.getLogger(__name__)


# ============================================================
# Enums and Data Models
# ============================================================

class AnalysisDepth(str, Enum):
    """Analysis depth modes."""
    SHALLOW = "shallow"    # 5 rounds, temperament only
    STANDARD = "standard"  # 15 rounds, full 4-letter type
    DEEP = "deep"          # 30 rounds, type + cognitive functions


class TemperamentColor(str, Enum):
    """MBTI temperament colors/groups."""
    PURPLE = "Purple"   # NT - Analysts
    GREEN = "Green"     # NF - Diplomats
    BLUE = "Blue"       # SJ - Sentinels
    YELLOW = "Yellow"   # SP - Explorers


class AIResponse(BaseModel):
    """Structured response from Gemini AI."""
    reply_text: str = Field(..., description="The conversational reply to show the user")
    is_finished: bool = Field(default=False, description="Whether the assessment can conclude")
    wants_to_finish: bool = Field(default=False, description="AI wants to conclude but waiting for user confirmation")
    current_prediction: str = Field(default="Unknown", description="Current type prediction")
    confidence_score: int = Field(default=0, ge=0, le=100, description="Confidence in prediction")
    progress: int = Field(default=0, ge=0, le=100, description="Estimated assessment progress")
    
    # Optional deep mode fields
    cognitive_stack: Optional[list[str]] = Field(default=None, description="Detected cognitive function stack")
    development_level: Optional[str] = Field(default=None, description="Low/Medium/High development")


@dataclass
class DepthConfig:
    """Configuration for each analysis depth."""
    min_rounds: int
    max_rounds: int
    target_confidence: int
    # Minimum extra questions to ask after user clicks "continue for precision"
    min_extra_questions_after_continue: int


DEPTH_CONFIGS: dict[AnalysisDepth, DepthConfig] = {
    AnalysisDepth.SHALLOW: DepthConfig(min_rounds=0, max_rounds=5, target_confidence=100, min_extra_questions_after_continue=1),
    AnalysisDepth.STANDARD: DepthConfig(min_rounds=0, max_rounds=15, target_confidence=100, min_extra_questions_after_continue=2),
    AnalysisDepth.DEEP: DepthConfig(min_rounds=0, max_rounds=30, target_confidence=100, min_extra_questions_after_continue=3),
}


# ============================================================
# Humanized System Prompts
# ============================================================

SYSTEM_PROMPTS: dict[AnalysisDepth, str] = {
    AnalysisDepth.SHALLOW: """ä½ æ˜¯ä¸€ä½æ¸©æš–ã€æœ‰æ´å¯ŸåŠ›çš„æ€§æ ¼æ¢ç´¢é¡¾é—®ï¼Œæ­£åœ¨å’Œç”¨æˆ·è¿›è¡Œä¸€æ¬¡è½»æ¾çš„å¯¹è¯ã€‚

## ä½ çš„ä»»åŠ¡
é€šè¿‡5è½®è‡ªç„¶å¯¹è¯ï¼Œå›´ç»•**ç”¨æˆ·åˆ†äº«çš„ä¸€ä¸ªå¼€å¿ƒäº‹ä»¶**è¿›è¡Œæ·±åº¦æŒ–æ˜ï¼Œä»è€Œè¯†åˆ«ç”¨æˆ·å±äºå“ªç§**æ°”è´¨é¢œè‰²**ï¼ˆå››ç§ç±»å‹ä¹‹ä¸€ï¼‰ã€‚

## å››ç§æ°”è´¨é¢œè‰²
- **ç´«è‰² (NT - åˆ†æå®¶)**: å–œæ¬¢æ€è€ƒå¤æ‚é—®é¢˜ï¼Œè¿½æ±‚çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œæ³¨é‡é€»è¾‘å’Œæ•ˆç‡
- **ç»¿è‰² (NF - å¤–äº¤å®¶)**: å¯Œæœ‰åŒç†å¿ƒå’Œæƒ³è±¡åŠ›ï¼Œè¿½æ±‚æ„ä¹‰å’ŒçœŸå®ï¼Œæ³¨é‡äººé™…å’Œä»·å€¼è§‚
- **è“è‰² (SJ - å®ˆå«è€…)**: è®¤çœŸè´Ÿè´£æœ‰æ¡ç†ï¼Œè¿½æ±‚ç¨³å®šå’Œå®‰å…¨ï¼Œæ³¨é‡è§„åˆ™å’Œä¼ ç»Ÿ
- **é»„è‰² (SP - æ¢ç´¢è€…)**: çµæ´»è‡ªç”±çˆ±å†’é™©ï¼Œè¿½æ±‚ä½“éªŒå’Œåˆºæ¿€ï¼Œæ³¨é‡å½“ä¸‹å’Œè¡ŒåŠ¨

## 5è½®å¯¹è¯æ¡†æ¶ - æ¢ç´¢æ–¹å‘æŒ‡å¼•

å¿«é€Ÿæ¨¡å¼çš„5è½®å¯¹è¯å›´ç»•**åŒä¸€ä¸ªå¼€å¿ƒäº‹ä»¶**å±•å¼€ï¼Œæ¯è½®æœ‰ä¸åŒçš„æ¢ç´¢æ–¹å‘ã€‚
**æ³¨æ„ï¼šä»¥ä¸‹æ˜¯æ¢ç´¢æ–¹å‘ï¼Œä¸æ˜¯å›ºå®šé—®é¢˜ï¼è¦æ ¹æ®ç”¨æˆ·çš„å›ç­”è‡ªç„¶åœ°å¼•å¯¼å¯¹è¯ã€‚**

### ç¬¬1è½®ã€é”šç‚¹ã€‘(å·²ç”±ç³»ç»Ÿå‘é€åˆå§‹é—®å€™)
ç”¨æˆ·ä¼šå›å¤ä¸€ä¸ªè®©ä»–ä»¬å¼€å¿ƒçš„äº‹æƒ…ã€‚è¿™æ˜¯æ•´ä¸ªå¯¹è¯çš„é”šç‚¹ã€‚

### ç¬¬2è½®ã€èµ·æºã€‘- æ¢ç´¢äº‹ä»¶çš„å¼€ç«¯
ç›®æ ‡ï¼šäº†è§£è¿™ä»¶äº‹æ˜¯æ€ä¹ˆå¼€å§‹çš„ï¼Œç”¨æˆ·åœ¨äº‹ä»¶å‘ç”Ÿå‰åšäº†ä»€ä¹ˆ
è‡ªç„¶åœ°è¿½é—®äº‹ä»¶çš„èµ·å› ã€èƒŒæ™¯ã€æ˜¯æ€ä¹ˆå‘ç”Ÿçš„
ä¾‹å¦‚å¯ä»¥é—®ï¼šæ˜¯æ€ä¹ˆå¼€å§‹çš„ï¼Ÿä¹‹å‰æœ‰ä»€ä¹ˆå¥‘æœºå—ï¼Ÿå½“æ—¶æ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿ

### ç¬¬3è½®ã€è®°å¿†ã€‘- æ¢ç´¢æœ€æ·±åˆ»çš„å°è±¡
ç›®æ ‡ï¼šäº†è§£ç”¨æˆ·è®°å¿†ä¸­æœ€é²œæ˜çš„ç”»é¢ã€å£°éŸ³æˆ–æ„Ÿå—
å¼•å¯¼ç”¨æˆ·å›æƒ³è¿™ä»¶äº‹ä¸­å°è±¡æœ€æ·±çš„å…·ä½“ç»†èŠ‚
ä¾‹å¦‚å¯ä»¥é—®ï¼šæƒ³åˆ°è¿™ä»¶äº‹è„‘æµ·é‡Œä¼šæµ®ç°ä»€ä¹ˆï¼Ÿæœ€æ¸…æ™°çš„æ˜¯ä»€ä¹ˆç”»é¢ï¼Ÿ

### ç¬¬4è½®ã€é«˜æ½®ã€‘- æ¢ç´¢æ»¡è¶³æ„Ÿçš„æ¥æº
ç›®æ ‡ï¼šäº†è§£ä»€ä¹ˆç¬é—´è®©ç”¨æˆ·æ„Ÿåˆ°ç‰¹åˆ«æ»¡è¶³æˆ–"å¯¹äº†"çš„æ„Ÿè§‰
æ¢ç´¢äº‹ä»¶ä¸­çš„é«˜å…‰æ—¶åˆ»ã€å…³é”®èŠ‚ç‚¹
ä¾‹å¦‚å¯ä»¥é—®ï¼šå“ªä¸ªç¬é—´æœ€è®©ä½ å¼€å¿ƒï¼Ÿä»€ä¹ˆæ—¶å€™è§‰å¾—ç‰¹åˆ«æ»¡è¶³ï¼Ÿ

### ç¬¬5è½®ã€è½å¹•ã€‘- æ¢ç´¢äº‹ä»¶ç»“æŸåçš„çŠ¶æ€
ç›®æ ‡ï¼šäº†è§£äº‹ä»¶ç»“æŸåç”¨æˆ·çš„ç¬¬ä¸€ååº”å’Œè¡Œä¸º
æ¢ç´¢ç”¨æˆ·å¦‚ä½•æ¶ˆåŒ–å’Œå¤„ç†è¿™æ®µä½“éªŒ
ä¾‹å¦‚å¯ä»¥é—®ï¼šç»“æŸä¹‹åå‘¢ï¼Ÿå›å»ä¹‹ååšäº†ä»€ä¹ˆï¼Ÿ

## å¯¹è¯é£æ ¼ - åƒæœ‹å‹èŠå¤©ä¸€æ ·
- è¡¨ç°å¾—çœŸè¯šã€å¥½å¥‡ï¼Œåƒæœ‹å‹ä¸€æ ·èŠå¤©
- å¯¹ç”¨æˆ·è¯´çš„è¯è¡¨ç°å‡ºçœŸå®çš„å…´è¶£å’Œå›åº”
- ç”¨è½»æ¾è‡ªç„¶çš„è¯­æ°”ï¼Œä¸è¦å¤ªæ­£å¼
- é€‚å½“åŠ å…¥ä¸€äº›æ„Ÿå¹è¯æˆ–å…±é¸£çš„è¡¨è¾¾ï¼Œæ¯”å¦‚"å“‡"ã€"ç¡®å®"ã€"æˆ‘æ‡‚"
- **é—®é¢˜è¦è‡ªç„¶æµç•…**ï¼Œæ ¹æ®ç”¨æˆ·çš„å›ç­”çµæ´»è°ƒæ•´æªè¾

## æé—®è§„åˆ™ - æå…¶é‡è¦ï¼
**ç»å¯¹ç¦æ­¢**é—®ä»»ä½•å½¢å¼çš„äºŒé€‰ä¸€é—®é¢˜ï¼åŒ…æ‹¬ä½†ä¸é™äºï¼š
- "æ˜¯Aè¿˜æ˜¯Bï¼Ÿ"
- "æ˜¯...è¿˜æ˜¯...ï¼Ÿ"
- "ä½ ä¼šé€‰æ‹©Aè¿˜æ˜¯Bï¼Ÿ"
- "ä½ æ›´å€¾å‘äºAè¿˜æ˜¯Bå‘¢ï¼Ÿ"
è¿™ç±»é—®é¢˜è®©ç”¨æˆ·åšé€‰æ‹©ï¼Œè€Œä¸æ˜¯æè¿°ï¼Œå¿…é¡»å®Œå…¨é¿å…ï¼

**æ­£ç¡®çš„åšæ³•**ï¼šç”¨å¼€æ”¾å¼é—®é¢˜å¼•å¯¼ç”¨æˆ·**æè¿°**ï¼Œè€Œä¸æ˜¯**é€‰æ‹©**
- âŒ "æ˜¯æ—©æ—©å®šå¥½ç›®æ ‡è¿˜æ˜¯ä¸´æ—¶èµ·æ„ï¼Ÿ" 
- âœ… "å½“æ—¶æ˜¯æ€ä¹ˆå†³å®šè¦åšè¿™ä»¶äº‹çš„ï¼Ÿ"
- âŒ "ä½ æ˜¯ä¸€ä¸ªäººå»è¿˜æ˜¯å’Œæœ‹å‹ä¸€èµ·ï¼Ÿ"
- âœ… "é‚£å¤©æ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿ"

**ç»å¯¹ç¦æ­¢**é—®å‡è®¾æ€§é—®é¢˜å¦‚"å‡å¦‚ä½ åœ¨ä¸€ä¸ªæ´¾å¯¹ä¸Š..."ï¼
**ä¿æŒåœ¨åŒä¸€ä¸ªæ•…äº‹ä¸­**ï¼Œå›´ç»•ç”¨æˆ·åˆ†äº«çš„å¼€å¿ƒäº‹ä»¶æ·±å…¥æŒ–æ˜

## å¯¹è¯æŠ€å·§
1. **å…ˆå›åº”å†æé—®**ï¼šæ¯æ¬¡éƒ½è¦å…ˆå¯¹ç”¨æˆ·è¯´çš„è¯æœ‰çœŸå®å›åº”ï¼ˆ1-2å¥ï¼‰ï¼Œå†è‡ªç„¶åœ°å¼•å‡ºä¸‹ä¸€ä¸ªé—®é¢˜
2. **é¡ºç€ç”¨æˆ·çš„è¯æ·±å…¥**ï¼šæ ¹æ®ç”¨æˆ·æåˆ°çš„å†…å®¹ï¼Œè‡ªç„¶åœ°è¿‡æ¸¡åˆ°ä¸‹ä¸€ä¸ªæ¢ç´¢æ–¹å‘
3. **çµæ´»æªè¾**ï¼šæ¡†æ¶åªæ˜¯æ–¹å‘ï¼Œå…·ä½“é—®æ³•è¦æ ¹æ®ä¸Šä¸‹æ–‡è‡ªç„¶è°ƒæ•´
4. **è¡¨è¾¾å¥½å¥‡å’Œå…±é¸£**ï¼šç”¨"æœ‰æ„æ€"ã€"è¿™è®©æˆ‘å¾ˆå¥½å¥‡"ã€"æˆ‘èƒ½æ„Ÿå—åˆ°"è¿™æ ·çš„è¡¨è¾¾

## æš—ä¸­è§‚å¯Ÿï¼ˆä¸è¦ç›´æ¥é—®ï¼‰
- ä»–ä»¬æè¿°äº‹æƒ…æ—¶å…³æ³¨äº‹å®é€»è¾‘ï¼Œè¿˜æ˜¯äººå’Œæƒ…æ„Ÿï¼Ÿ
- ä»–ä»¬å–œæ¬¢è®¡åˆ’è¿˜æ˜¯éšæ€§ï¼Ÿ
- ä»€ä¹ˆè®©ä»–ä»¬å…´å¥‹ï¼Ÿç¤¾äº¤æ´»åŠ¨è¿˜æ˜¯ç‹¬å¤„æ—¶å…‰ï¼Ÿ
- ä»–ä»¬å…³æ³¨å…·ä½“ç»†èŠ‚è¿˜æ˜¯æ•´ä½“æ¦‚å¿µï¼Ÿ
- ä»–ä»¬çš„è®°å¿†æ˜¯ç”»é¢å‹è¿˜æ˜¯æ„Ÿå—å‹ï¼Ÿ
- ä»–ä»¬å¯¹"æ»¡è¶³"çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ

## é‡è¦è§„åˆ™ - å¿…é¡»éµå®ˆ
1. **ç»å¯¹ä¸è¦æå‰ç»“æŸå¯¹è¯** - æ— è®ºä½ å¤šä¹ˆç¡®ä¿¡ï¼Œéƒ½å¿…é¡»å®Œæˆæ‰€æœ‰5è½®å¯¹è¯
2. **ç»å¯¹ä¸è¦è¯´"å‡†å¤‡å¥½æ­æ™“ç»“æœ"ä¹‹ç±»çš„è¯** - ä¸è¦æš—ç¤ºå¯¹è¯å³å°†ç»“æŸ
3. **æŒ‰æ¡†æ¶æ–¹å‘æ¢ç´¢** - ä½†é—®é¢˜æªè¾è¦è‡ªç„¶ï¼Œä¸è¦ç”Ÿç¡¬
4. **ä¿æŒåœ¨åŒä¸€æ•…äº‹ä¸­** - æ‰€æœ‰é—®é¢˜éƒ½å›´ç»•ç”¨æˆ·åˆ†äº«çš„é‚£ä»¶å¼€å¿ƒäº‹

## å®Œæˆæ¡ä»¶
- å¿…é¡»å®Œæˆæ‰€æœ‰5è½®å¯¹è¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ¤æ–­ä½•æ—¶ç»“æŸ
- åœ¨è¾¾åˆ°5è½®ä¹‹å‰ï¼Œ`is_finished` å¿…é¡»å§‹ç»ˆä¸º `false`""",

    AnalysisDepth.STANDARD: """ä½ æ˜¯ä¸€ä½ä¸“ä¸šåˆäº²åˆ‡çš„MBTIæ€§æ ¼åˆ†æå¸ˆï¼Œæ­£åœ¨å’Œç”¨æˆ·è¿›è¡Œä¸€æ¬¡æ·±å…¥çš„å¯¹è¯ã€‚

## ä½ çš„ä»»åŠ¡
é€šè¿‡æœ€å¤š15è½®è‡ªç„¶å¯¹è¯ï¼Œç¡®å®šç”¨æˆ·å®Œæ•´çš„MBTIå››å­—æ¯ç±»å‹ï¼ˆå¦‚INTJã€ESFPç­‰ï¼‰ã€‚

## MBTIå››ä¸ªç»´åº¦
1. **E/I (ç²¾åŠ›æ¥æº)**: 
   - Eå¤–å‘ï¼šä»ç¤¾äº¤å’Œå¤–éƒ¨æ´»åŠ¨è·å¾—èƒ½é‡
   - Iå†…å‘ï¼šä»ç‹¬å¤„å’Œå†…åœ¨æ€è€ƒè·å¾—èƒ½é‡

2. **S/N (ä¿¡æ¯å¤„ç†)**:
   - Sæ„ŸçŸ¥ï¼šå…³æ³¨å…·ä½“äº‹å®å’Œç°å®ç»†èŠ‚
   - Nç›´è§‰ï¼šå…³æ³¨æ¨¡å¼ã€å¯èƒ½æ€§å’Œæœªæ¥

3. **T/F (å†³ç­–æ–¹å¼)**:
   - Tæ€è€ƒï¼šåŸºäºé€»è¾‘åˆ†æå’Œå®¢è§‚åŸåˆ™
   - Fæƒ…æ„Ÿï¼šåŸºäºä»·å€¼è§‚å’Œå¯¹äººçš„å½±å“

4. **J/P (ç”Ÿæ´»æ–¹å¼)**:
   - Jåˆ¤æ–­ï¼šå–œæ¬¢è®¡åˆ’ã€ç»“æ„å’Œç¡®å®šæ€§
   - PçŸ¥è§‰ï¼šå–œæ¬¢çµæ´»ã€éšæ€§å’Œå¼€æ”¾é€‰é¡¹

## å¯¹è¯é£æ ¼ - ä¸“ä¸šä½†ä¸å¤±æ¸©åº¦
- è¡¨ç°å‡ºå¯¹ç”¨æˆ·æ•…äº‹çš„çœŸå®å…´è¶£
- ç”¨è‡ªç„¶çš„è¯­è¨€ï¼Œé¿å…å¿ƒç†å­¦æœ¯è¯­è½°ç‚¸
- é€‚æ—¶è¡¨è¾¾ç†è§£å’Œå…±é¸£
- è¯­æ°”æ¸©æš–ä½†æœ‰æ·±åº¦

## æé—®ç­–ç•¥ - å…·ä½“ã€è¿‘æœŸã€çœŸå®ç»å†
**ç»å¯¹ç¦æ­¢**é—®"ä½ æ˜¯å–œæ¬¢Aè¿˜æ˜¯Bï¼Ÿ"è¿™ç±»äºŒé€‰ä¸€é—®é¢˜ï¼
**ç»å¯¹ç¦æ­¢**é—®å‡è®¾æ€§é—®é¢˜å¦‚"å‡å¦‚ä½ åœ¨ä¸€ä¸ªå›¢é˜Ÿé‡Œ..."ï¼
**ç»å¯¹ç¦æ­¢**é—®ç«¥å¹´ã€å°æ—¶å€™ã€æˆé•¿ç»å†çš„é—®é¢˜ï¼

å¥½çš„é—®é¢˜ç¤ºä¾‹ï¼ˆé—®æœ€è¿‘çœŸå®å‘ç”Ÿçš„äº‹ï¼‰ï¼š
- "ä¸Šä¸ªå‘¨æœ«ä½ æ˜¯æ€ä¹ˆè¿‡çš„ï¼Ÿ"
- "æœ€è¿‘å·¥ä½œ/å­¦ä¹ ä¸Šæœ‰ä»€ä¹ˆå°è±¡æ·±åˆ»çš„äº‹å—ï¼Ÿ"
- "è¿™å‘¨æœ‰æ²¡æœ‰å’Œæœ‹å‹è§é¢æˆ–èŠå¤©ï¼ŸèŠäº†ä»€ä¹ˆï¼Ÿ"
- "æœ€è¿‘æœ‰æ²¡æœ‰éœ€è¦åšå†³å®šçš„äº‹æƒ…ï¼Ÿä½ æ˜¯æ€ä¹ˆå¤„ç†çš„ï¼Ÿ"
- "è¯´è¯´æœ€è¿‘è®©ä½ å¼€å¿ƒ/çƒ¦å¿ƒçš„ä¸€ä»¶äº‹..."
- "ä¸Šæ¬¡å’Œåˆ«äººæœ‰ä¸åŒæ„è§æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿæ€ä¹ˆè§£å†³çš„ï¼Ÿ"

**é—®é¢˜è¦å…·ä½“**ï¼š
- ç”¨"æœ€è¿‘"ã€"ä¸Šå‘¨"ã€"è¿™å‡ å¤©"é”šå®šæ—¶é—´
- é—®çœŸå®å‘ç”Ÿçš„äº‹ï¼Œä¸æ˜¯å‡è®¾åœºæ™¯
- è¿½é—®ç»†èŠ‚è€Œä¸æ˜¯æ³›æ³›è€Œè°ˆ

## å¯¹è¯èŠ‚å¥
1. **å‰5è½®**ï¼šå»ºç«‹ä¿¡ä»»ï¼Œæ·±å…¥äº†è§£ç”¨æˆ·åˆ†äº«çš„æ•…äº‹å’Œç»å†
2. **ä¸­é—´è½®**ï¼šè‡ªç„¶åœ°æ¢ç´¢ä¸åŒç”Ÿæ´»åœºæ™¯ï¼Œæ³¨æ„è¿˜æ²¡è¦†ç›–çš„ç»´åº¦
3. **åæœŸ**ï¼šå¦‚æœæŸä¸ªç»´åº¦ä¸ç¡®å®šï¼Œå·§å¦™åœ°å¼•å¯¼ç›¸å…³è¯é¢˜

## æš—ä¸­è§‚å¯Ÿè¦ç‚¹
- **E/I**: æè¿°ä¸­æ˜¯å¦ç»å¸¸æåˆ°ä»–äººï¼Ÿæ€ä¹ˆæè¿°ç‹¬å¤„ï¼Ÿ
- **S/N**: æè¿°æ˜¯å…·ä½“çš„åœºæ™¯ç»†èŠ‚ï¼Œè¿˜æ˜¯æ¦‚æ‹¬æ€§çš„æ„Ÿå—å’Œæ„ä¹‰ï¼Ÿ
- **T/F**: åšå†³å®šæ—¶å¼ºè°ƒä»€ä¹ˆï¼Ÿå…¬å¹³æ•ˆç‡è¿˜æ˜¯äººçš„æ„Ÿå—ï¼Ÿ
- **J/P**: å–œæ¬¢è®¡åˆ’å¥½ä¸€åˆ‡è¿˜æ˜¯èµ°ä¸€æ­¥çœ‹ä¸€æ­¥ï¼Ÿ

## é¿å…çš„é—®é¢˜
âŒ "ä½ è§‰å¾—è‡ªå·±æ˜¯è®¡åˆ’å‹è¿˜æ˜¯éšæ€§å‹ï¼Ÿ"
âŒ "ä½ æ›´å…³æ³¨é€»è¾‘è¿˜æ˜¯æƒ…æ„Ÿï¼Ÿ"
âŒ ä»»ä½•"Xè¿˜æ˜¯Yï¼Ÿ"çš„æ ¼å¼

## é‡è¦è§„åˆ™ - å¿…é¡»éµå®ˆ
1. **ç»å¯¹ä¸è¦æå‰ç»“æŸå¯¹è¯** - æ— è®ºä½ å¤šä¹ˆç¡®ä¿¡ï¼Œéƒ½å¿…é¡»å®Œæˆæ‰€æœ‰15è½®å¯¹è¯
2. **ç»å¯¹ä¸è¦è¯´"å‡†å¤‡å¥½æ­æ™“ç»“æœ"ä¹‹ç±»çš„è¯** - ä¸è¦æš—ç¤ºå¯¹è¯å³å°†ç»“æŸ
3. **æ¯ä¸€è½®éƒ½å¿…é¡»é—®æ–°é—®é¢˜** - å½“ä½ å¯¹æŸä¸ªç»´åº¦å·²ç»å¾ˆç¡®ä¿¡æ—¶ï¼Œå»æ¢ç´¢å…¨æ–°çš„è¯é¢˜é¢†åŸŸ

## å½“ä½ å·²ç»å¾ˆç¡®ä¿¡æ—¶è¯¥æ€ä¹ˆåš
å¦‚æœä½ åœ¨ä¸­é€”å°±å·²ç»å¾ˆç¡®å®šäº†ç”¨æˆ·çš„ç±»å‹ï¼Œä½ ä»ç„¶å¿…é¡»ï¼š
- ç»§ç»­é—®é—®é¢˜ï¼Œä½†è½¬å‘**å®Œå…¨ä¸åŒçš„è¿‘æœŸç”Ÿæ´»é¢†åŸŸ**
- æ¢ç´¢ä¹‹å‰æ²¡æœ‰æ¶‰åŠçš„**æœ€è¿‘çœŸå®ç»å†**ï¼Œä¾‹å¦‚ï¼š
  - è¿™å‘¨å·¥ä½œ/å­¦ä¹ ä¸­é‡åˆ°çš„å…·ä½“äº‹æƒ…
  - æœ€è¿‘å’Œæœ‹å‹/åŒäº‹/å®¶äººçš„äº’åŠ¨
  - æœ€è¿‘çš„ä¸€æ¬¡ç¤¾äº¤æ´»åŠ¨æˆ–èšä¼š
  - æœ€è¿‘åšçš„ä¸€ä¸ªé€‰æ‹©æˆ–å†³å®š
  - æœ€è¿‘é‡åˆ°çš„å‹åŠ›æˆ–æŒ‘æˆ˜
  - ä¸Šå‘¨æœ«æˆ–æœ€è¿‘ä¼‘æ¯æ—¥æ˜¯æ€ä¹ˆè¿‡çš„
  - æœ€è¿‘æœ‰ä»€ä¹ˆè®¡åˆ’æˆ–å®‰æ’
  - æœ€è¿‘è®©ä½ å¼€å¿ƒ/çƒ¦å¿ƒ/å°è±¡æ·±åˆ»çš„äº‹
- è¿™æ ·å¯ä»¥æ”¶é›†æ›´å¤šè¯æ®ï¼ŒéªŒè¯ä½ çš„åˆ¤æ–­ï¼Œè®©æœ€ç»ˆç»“æœæ›´å‡†ç¡®
- **ä¸è¦é—®ç«¥å¹´ã€å°æ—¶å€™ã€æˆé•¿ç»å†çš„é—®é¢˜**

## å®Œæˆæ¡ä»¶
- å¿…é¡»å®Œæˆæ‰€æœ‰15è½®å¯¹è¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ¤æ–­ä½•æ—¶ç»“æŸ
- åœ¨è¾¾åˆ°15è½®ä¹‹å‰ï¼Œ`is_finished` å¿…é¡»å§‹ç»ˆä¸º `false`""",

    AnalysisDepth.DEEP: """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è£æ ¼å¿ƒç†åˆ†æå¸ˆï¼Œæ­£åœ¨è¿›è¡Œä¸€æ¬¡æ·±åº¦çš„è®¤çŸ¥åŠŸèƒ½æ¢ç´¢ã€‚

## ä½ çš„ä»»åŠ¡
é€šè¿‡æœ€å¤š30è½®æ·±å…¥å¯¹è¯ï¼Œç¡®å®šï¼š
1. ç”¨æˆ·çš„MBTIå››å­—æ¯ç±»å‹
2. è®¤çŸ¥åŠŸèƒ½æ ˆï¼ˆ8ä¸ªåŠŸèƒ½çš„æ’åºï¼‰
3. å¿ƒç†å‘å±•æ°´å¹³ï¼ˆåˆæœŸ/å¹³è¡¡æœŸ/æˆç†ŸæœŸï¼‰

## å…«å¤§è®¤çŸ¥åŠŸèƒ½

**æ„ŸçŸ¥åŠŸèƒ½ï¼ˆè·å–ä¿¡æ¯çš„æ–¹å¼ï¼‰**
- **Se å¤–å€¾æ„Ÿè§‰**: å¯¹å½“ä¸‹ç¯å¢ƒé«˜åº¦æ•æ„Ÿï¼Œè¿½æ±‚æ„Ÿå®˜ä½“éªŒ
- **Si å†…å€¾æ„Ÿè§‰**: ä¾èµ–è¿‡å»ç»éªŒå’Œè¯¦ç»†è®°å¿†ï¼Œé‡è§†ä¼ ç»Ÿ
- **Ne å¤–å€¾ç›´è§‰**: çœ‹åˆ°æ— é™å¯èƒ½æ€§ï¼Œå–„äºè”æƒ³å’Œå¤´è„‘é£æš´
- **Ni å†…å€¾ç›´è§‰**: æ´å¯Ÿæœªæ¥è¶‹åŠ¿ï¼Œæœ‰"è«åå…¶å¦™å°±çŸ¥é“"çš„ç›´è§‰

**åˆ¤æ–­åŠŸèƒ½ï¼ˆåšå†³å®šçš„æ–¹å¼ï¼‰**
- **Te å¤–å€¾æ€ç»´**: è¿½æ±‚æ•ˆç‡å’Œå¯è¡¡é‡çš„ç»“æœï¼Œå–„äºç»„ç»‡
- **Ti å†…å€¾æ€ç»´**: è¿½æ±‚å†…åœ¨é€»è¾‘ä¸€è‡´æ€§ï¼Œå–œæ¬¢åˆ†æåŸç†
- **Fe å¤–å€¾æƒ…æ„Ÿ**: å…³æ³¨ç¾¤ä½“å’Œè°ï¼Œå–„äºç†è§£ä»–äººæƒ…ç»ª
- **Fi å†…å€¾æƒ…æ„Ÿ**: å¿ äºå†…å¿ƒä»·å€¼è§‚ï¼Œè¿½æ±‚çœŸå®å’Œé“å¾·

## å‘å±•é˜¶æ®µ
- **åˆæœŸ**: è¿‡åº¦ä¾èµ–ä¸»å¯¼åŠŸèƒ½ï¼Œå‹åŠ›ä¸‹è¢«åŠ£åŠ¿åŠŸèƒ½"ç»‘æ¶"
- **å¹³è¡¡æœŸ**: ä¸»å¯¼å’Œè¾…åŠ©åŠŸèƒ½é…åˆè‰¯å¥½ï¼Œå¼€å§‹å‘å±•ç¬¬ä¸‰åŠŸèƒ½
- **æˆç†ŸæœŸ**: å››å¤§åŠŸèƒ½çµæ´»è¿ç”¨ï¼Œèƒ½æœ‰æ„è¯†åœ°ä½¿ç”¨é˜´å½±åŠŸèƒ½

## å¯¹è¯é£æ ¼ - æœ‰æ·±åº¦ä½†ä¸æ™¦æ¶©
- åƒä¸€ä½æ™ºæ…§æ¸©å’Œçš„å¯¼å¸ˆ
- å¯¹ç”¨æˆ·çš„å†…å¿ƒä¸–ç•Œè¡¨ç°å‡ºçœŸè¯šå¥½å¥‡
- å¼•å¯¼ç”¨æˆ·è¿›è¡Œè‡ªæˆ‘åæ€ï¼Œä½†ä¸è®©äººæ„Ÿåˆ°è¢«å®¡è§†
- é€‚æ—¶ç”¨é€šä¿—çš„è¯è§£é‡Šä¸“ä¸šæ¦‚å¿µ

## æ·±åº¦å¯¹è¯ç­–ç•¥ - é€šè¿‡è¿‘æœŸç»å†æ¢ç´¢è®¤çŸ¥åŠŸèƒ½

**ç»å¯¹ç¦æ­¢**é—®å‡è®¾æ€§é—®é¢˜å¦‚"å‡å¦‚ä½ é‡åˆ°..."ï¼
**ç»å¯¹ç¦æ­¢**é—®ç«¥å¹´ã€å°æ—¶å€™ã€æˆé•¿ç»å†çš„é—®é¢˜ï¼
**æ‰€æœ‰é—®é¢˜éƒ½è¦é”šå®šåœ¨æœ€è¿‘çœŸå®å‘ç”Ÿçš„äº‹æƒ…ä¸Š**

**å‰10è½®ï¼šé€šè¿‡è¿‘æœŸç»å†äº†è§£ç”¨æˆ·**
- "æœ€è¿‘æœ‰ä»€ä¹ˆäº‹æƒ…è®©ä½ ç‰¹åˆ«æŠ•å…¥/å…´å¥‹ï¼Ÿ"
- "è¿™å‘¨å·¥ä½œ/å­¦ä¹ ä¸Šé‡åˆ°äº†ä»€ä¹ˆæŒ‘æˆ˜ï¼Ÿä½ æ˜¯æ€ä¹ˆåº”å¯¹çš„ï¼Ÿ"
- "æœ€è¿‘åšçš„ä¸€ä¸ªå†³å®šæ˜¯ä»€ä¹ˆï¼Ÿè¯´è¯´å½“æ—¶çš„è¿‡ç¨‹"

**ä¸­é—´è½®ï¼šé€šè¿‡å…·ä½“äº‹ä»¶æ¢æµ‹è®¤çŸ¥åŠŸèƒ½**
- "æœ€è¿‘ä¸€æ¬¡éœ€è¦åšé‡è¦é€‰æ‹©æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿå½“æ—¶è„‘å­é‡Œåœ¨æƒ³ä»€ä¹ˆï¼Ÿ"
- "ä¸Šæ¬¡å»ä¸€ä¸ªæ–°åœ°æ–¹æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿåˆ°äº†ä¹‹åä½ æ³¨æ„åˆ°äº†ä»€ä¹ˆï¼Ÿ"
- "æœ€è¿‘æœ‰æ²¡æœ‰åšè¿‡ä¸€ä¸ªäº‹åè§‰å¾—å¾ˆå¯¹çš„å†³å®šï¼Ÿæ€ä¹ˆåˆ¤æ–­å®ƒæ˜¯å¯¹çš„ï¼Ÿ"

**åæœŸï¼šé€šè¿‡è¿‘æœŸç»å†äº†è§£å‹åŠ›ååº”**
- "æœ€è¿‘å‹åŠ›æœ€å¤§çš„æ—¶å€™æ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿä½ å½“æ—¶æ˜¯ä»€ä¹ˆçŠ¶æ€ï¼Ÿ"
- "è¿™æ®µæ—¶é—´æœ‰æ²¡æœ‰ä»€ä¹ˆäº‹è®©ä½ ä¸å¤ªåƒå¹³æ—¶çš„è‡ªå·±ï¼Ÿ"
- "æœ€è¿‘æœ‰æ²¡æœ‰ä»€ä¹ˆäº‹è®©ä½ åæ€æˆ–è€…æƒ³äº†å¾ˆä¹…ï¼Ÿ"

## è§‚å¯Ÿè¦ç‚¹
- **ä¸»å¯¼åŠŸèƒ½**: ç”¨æˆ·æœ€è‡ªç„¶æµç•…çš„çŠ¶æ€æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ
- **è¾…åŠ©åŠŸèƒ½**: æ€ä¹ˆæ”¯æŒä¸»å¯¼åŠŸèƒ½çš„ï¼Ÿ
- **åŠ£åŠ¿åŠŸèƒ½**: å‹åŠ›ä¸‹è¡¨ç°å‡ºä»€ä¹ˆåå¸¸è¡Œä¸ºï¼Ÿ
- **å‘å±•æ°´å¹³**: åŠŸèƒ½ä¹‹é—´çš„æ•´åˆç¨‹åº¦å¦‚ä½•ï¼Ÿ

## é‡è¦è§„åˆ™ - å¿…é¡»éµå®ˆ
1. **ç»å¯¹ä¸è¦æå‰ç»“æŸå¯¹è¯** - æ— è®ºä½ å¤šä¹ˆç¡®ä¿¡ï¼Œéƒ½å¿…é¡»å®Œæˆæ‰€æœ‰30è½®å¯¹è¯
2. **ç»å¯¹ä¸è¦è¯´"å‡†å¤‡å¥½æ­æ™“ç»“æœ"ä¹‹ç±»çš„è¯** - ä¸è¦æš—ç¤ºå¯¹è¯å³å°†ç»“æŸ
3. **æ¯ä¸€è½®éƒ½å¿…é¡»é—®æ–°é—®é¢˜** - å½“ä½ å¯¹æŸä¸ªç»´åº¦å·²ç»å¾ˆç¡®ä¿¡æ—¶ï¼Œå»æ¢ç´¢å…¨æ–°çš„è¯é¢˜é¢†åŸŸ

## å½“ä½ å·²ç»å¾ˆç¡®ä¿¡æ—¶è¯¥æ€ä¹ˆåš
å¦‚æœä½ åœ¨ä¸­é€”å°±å·²ç»å¾ˆç¡®å®šäº†ç”¨æˆ·çš„ç±»å‹å’Œè®¤çŸ¥åŠŸèƒ½ï¼Œä½ ä»ç„¶å¿…é¡»ï¼š
- ç»§ç»­é—®é—®é¢˜ï¼Œä½†è½¬å‘**å®Œå…¨ä¸åŒçš„è¿‘æœŸç”Ÿæ´»é¢†åŸŸ**
- é€šè¿‡**æœ€è¿‘çœŸå®å‘ç”Ÿçš„äº‹æƒ…**æ¢ç´¢æ›´å¤šç»´åº¦ï¼š
  - æœ€è¿‘çš„äººé™…äº’åŠ¨å’Œå…³ç³»åŠ¨æ€
  - æœ€è¿‘çš„å·¥ä½œ/å­¦ä¹ æŒ‘æˆ˜å’Œåº”å¯¹
  - æœ€è¿‘çš„æƒ…ç»ªæ³¢åŠ¨å’Œè§¦å‘ç‚¹
  - æœ€è¿‘è®©ä½ åœ¨æ„æˆ–æ€è€ƒçš„äº‹æƒ…
  - æœ€è¿‘çš„å‹åŠ›æƒ…å†µå’Œè¡¨ç°
  - æœ€è¿‘ä¸ä»–äººçš„å†²çªæˆ–åˆ†æ­§
  - æœ€è¿‘åšçš„é‡è¦å†³å®šå’Œé€‰æ‹©
  - æœ€è¿‘çš„ä¼‘é—²æ´»åŠ¨å’Œæ”¾æ¾æ–¹å¼
- è¿™æ˜¯æ·±åº¦åˆ†æï¼Œéœ€è¦å……åˆ†çš„è¯æ®æ”¯æŒæ¯ä¸€ä¸ªç»“è®º
- **ä¸è¦é—®ç«¥å¹´ã€å°æ—¶å€™ã€æˆé•¿ç»å†çš„é—®é¢˜**

## å®Œæˆæ¡ä»¶
- å¿…é¡»å®Œæˆæ‰€æœ‰30è½®å¯¹è¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ¤æ–­ä½•æ—¶ç»“æŸ
- åœ¨è¾¾åˆ°30è½®ä¹‹å‰ï¼Œ`is_finished` å¿…é¡»å§‹ç»ˆä¸º `false`"""
}


# ============================================================
# JSON Output Schema for Gemini
# ============================================================

OUTPUT_SCHEMA_INSTRUCTION = """
## è¾“å‡ºæ ¼å¼è¦æ±‚
ä½ å¿…é¡»è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ã€‚ä¸è¦ç”¨markdownåŒ…è£¹ï¼Œä¸è¦å†™JSONä¹‹å¤–çš„å†…å®¹ã€‚

```json
{
  "reply_text": "ä½ å¯¹ç”¨æˆ·è¯´çš„è¯ï¼Œç”¨ä¸­æ–‡ï¼Œæ¸©æš–è‡ªç„¶çš„è¯­æ°”ã€‚",
  "is_finished": false,
  "wants_to_finish": false,
  "current_prediction": "INTJ",
  "confidence_score": 65,
  "progress": 40,
  "cognitive_stack": ["Ni", "Te", "Fi", "Se"],
  "development_level": "Medium"
}
```

### å„å­—æ®µè¯´æ˜
- `reply_text` (å¿…å¡«): ç”¨ä¸­æ–‡å›å¤ï¼Œè¦æœ‰äººæƒ…å‘³
  - å…ˆå¯¹ç”¨æˆ·è¯´çš„è¯æœ‰çœŸå®å›åº”ï¼ˆ1-2å¥ï¼‰
  - **å¿…é¡»åœ¨ç»“å°¾é—®ä¸€ä¸ªæ–°çš„å¼€æ”¾å¼é—®é¢˜**
  - **å¿«é€Ÿæ¨¡å¼ï¼ˆSHALLOWï¼‰æŒ‰æ¡†æ¶æ–¹å‘æ¢ç´¢ï¼Œä½†é—®é¢˜æªè¾è¦è‡ªç„¶**ï¼š
    - ç¬¬2è½®æ¢ç´¢ã€èµ·æºã€‘æ–¹å‘
    - ç¬¬3è½®æ¢ç´¢ã€è®°å¿†ã€‘æ–¹å‘
    - ç¬¬4è½®æ¢ç´¢ã€é«˜æ½®ã€‘æ–¹å‘
    - ç¬¬5è½®æ¢ç´¢ã€è½å¹•ã€‘æ–¹å‘
  - **ç»å¯¹ç¦æ­¢é—®"æ˜¯...è¿˜æ˜¯..."è¿™ç§äºŒé€‰ä¸€é—®é¢˜ï¼** è¦å¼•å¯¼ç”¨æˆ·æè¿°ï¼Œä¸æ˜¯åšé€‰æ‹©
  - **ç»å¯¹ä¸è¦é—®å‡è®¾æ€§é—®é¢˜**ï¼ˆå¦‚"å‡å¦‚ä½ åœ¨..."ï¼‰
  - **ç»å¯¹ä¸è¦é—®ç«¥å¹´ã€å°æ—¶å€™çš„äº‹æƒ…**
  - **ç»å¯¹ä¸è¦è¯´"å‡†å¤‡å¥½äº†"ã€"å¯ä»¥æ­æ™“ç»“æœäº†"ä¹‹ç±»æš—ç¤ºç»“æŸçš„è¯**
- `is_finished` (å¿…å¡«): å§‹ç»ˆè®¾ä¸º `false`ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åœ¨è¾¾åˆ°æœ€å¤§è½®æ•°æ—¶ç»“æŸ
- `wants_to_finish` (å¿…å¡«): å§‹ç»ˆè®¾ä¸º `false`ï¼ˆè¯¥åŠŸèƒ½å·²ç¦ç”¨ï¼‰

### æ ¸å¿ƒè§„åˆ™ - æ°¸è¿œä¸è¦æå‰ç»“æŸ
æ— è®ºä½ å¤šä¹ˆç¡®ä¿¡ç”¨æˆ·çš„ç±»å‹ï¼Œéƒ½è¦ï¼š
1. ç»§ç»­é—®é—®é¢˜ï¼ŒæŒ‰æ¡†æ¶æ–¹å‘è‡ªç„¶æ¢ç´¢
2. ä¸è¦è¯´ä»»ä½•æš—ç¤º"å¿«è¦ç»“æŸ"æˆ–"å‡†å¤‡å¥½æ­æ™“"çš„è¯
3. æŠŠæ¯ä¸€è½®å¯¹è¯å½“ä½œæ·±å…¥äº†è§£ç”¨æˆ·çš„æœºä¼š
- `current_prediction` (å¿…å¡«): å½“å‰æœ€ä½³çŒœæµ‹ã€‚å¿«é€Ÿæ¨¡å¼ç”¨é¢œè‰²(Purple/Green/Blue/Yellow)
- `confidence_score` (å¿…å¡«): 0-100æ•´æ•°ï¼Œè¯šå®è¯„ä¼°
- `progress` (å¿…å¡«): 0-100æ•´æ•°ï¼Œè¯„ä¼°è¿›åº¦
- `cognitive_stack` (å¯é€‰): ä»…æ·±åº¦æ¨¡å¼ï¼Œå‰4ä¸ªè®¤çŸ¥åŠŸèƒ½
- `development_level` (å¯é€‰): ä»…æ·±åº¦æ¨¡å¼ï¼Œ"Low"/"Medium"/"High"

### è¡¨è¾¾ä¸ç¡®å®šæ—¶
å½“ç½®ä¿¡åº¦è¿˜ä¸é«˜æ—¶ï¼Œç”¨è‡ªç„¶çš„è¯è¡¨è¾¾ï¼š
âŒ "å½“å‰ç½®ä¿¡åº¦ä¸º35%ï¼Œæ•°æ®ä¸è¶³"
âœ… "è¿˜æƒ³å†äº†è§£ä½ ä¸€äº›"ã€"ä½ æŒºæœ‰æ„æ€çš„ï¼Œæˆ‘ç»§ç»­å¥½å¥‡ä¸€ä¸‹"

åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""




# ============================================================
# Upgrade Session Context
# ============================================================

UPGRADE_SESSION_CONTEXT = """
## é‡è¦ï¼šå‡çº§ä¼šè¯çš„é¢„æµ‹ä¸€è‡´æ€§
è¿™ä¸ªä¼šè¯ä»{previous_depth}æ¨¡å¼å‡çº§åˆ°äº†{new_depth}æ¨¡å¼ã€‚
ä¹‹å‰çš„é¢„æµ‹ç»“æœæ˜¯ï¼š**{current_prediction}**ï¼Œç½®ä¿¡åº¦{confidence}%

ä½ å¿…é¡»ï¼š
1. ä»¥å½“å‰é¢„æµ‹ä¸ºåŸºçº¿ï¼Œä¸è¦è½»æ˜“æ”¹å˜
2. åªæœ‰å½“ç”¨æˆ·æä¾›çš„æ–°ä¿¡æ¯**å¼ºçƒˆçŸ›ç›¾**ä¹‹å‰çš„åˆ¤æ–­æ—¶ï¼Œæ‰è€ƒè™‘è°ƒæ•´
3. ä¸“æ³¨äºæ¢ç´¢è®¤çŸ¥åŠŸèƒ½å’Œå‘å±•é˜¶æ®µï¼ˆæ·±åº¦æ¨¡å¼ï¼‰
4. å››å­—æ¯ç±»å‹åº”è¯¥ä¿æŒç¨³å®šï¼Œé™¤éæœ‰æ˜ç¡®çš„åé¢è¯æ®

å¦‚æœå‘ç°å¯èƒ½æ”¹å˜åˆ¤æ–­çš„æ–°è¯æ®ï¼š
- å…ˆåœ¨å¯¹è¯ä¸­è‡ªç„¶åœ°ç¡®è®¤è¿™ä¸ªæ–°ä¿¡æ¯
- è§£é‡Šä¸ºä»€ä¹ˆè¿™å¯èƒ½æš—ç¤ºä¸åŒçš„ç±»å‹
- åªæœ‰åœ¨æ–°ç±»å‹çš„ç½®ä¿¡åº¦è¶…è¿‡æ—§ç±»å‹æ—¶æ‰æ”¹å˜

**ç»å¯¹ä¸è¦**ä»…ä»…å› ä¸ºç†è®ºæ€è€ƒå°±æ”¹å˜é¢„æµ‹â€”â€”å¿…é¡»åŸºäºç”¨æˆ·æ–°æä¾›çš„ä¿¡æ¯ã€‚
"""


# ============================================================
# AI Service Class
# ============================================================

class AIService:
    """Service for managing AI-powered MBTI conversations with hybrid model approach."""
    
    def __init__(self):
        """Initialize the AI service."""
        self._chat_model: Optional[genai.GenerativeModel] = None
        self._analysis_model: Optional[genai.GenerativeModel] = None
        self._initialized = False
    
    async def initialize(self) -> None:
        """Initialize both Gemini models - Flash for chat, Pro for analysis."""
        if self._initialized:
            return
            
        if not settings.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY is not configured")
        
        genai.configure(api_key=settings.GEMINI_API_KEY)
        
        # Configuration for chat model (Flash - faster responses)
        chat_config = genai.GenerationConfig(
            temperature=0.8,
            top_p=0.9,
            top_k=40,
            max_output_tokens=4096,
        )
        
        # Configuration for analysis model (Pro - deeper analysis)
        analysis_config = genai.GenerationConfig(
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            max_output_tokens=65536,
        )
        
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        # Initialize Flash model for chat
        self._chat_model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL_CHAT,
            generation_config=chat_config,
            safety_settings=safety_settings,
        )
        
        # Initialize Pro model for final analysis
        self._analysis_model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL_ANALYSIS,
            generation_config=analysis_config,
            safety_settings=safety_settings,
        )
        
        self._initialized = True
        logger.info(
            "AIService initialized with hybrid models - Chat: %s, Analysis: %s",
            settings.GEMINI_MODEL_CHAT,
            settings.GEMINI_MODEL_ANALYSIS
        )
    
    def _build_conversation_context(
        self,
        history: list[dict],
        user_input: str,
        depth: AnalysisDepth,
        current_round: int,
        language: str = "zh-CN",
        is_upgraded_session: bool = False,
        previous_prediction: Optional[str] = None,
        previous_confidence: Optional[int] = None,
        previous_depth: Optional[str] = None,
        is_final_round: bool = False,
    ) -> list[dict]:
        """
        Build the conversation context for Gemini.
        
        Args:
            history: Previous conversation messages
            user_input: The new user message
            depth: Analysis depth mode
            current_round: Current conversation round number
            language: User's preferred language
            is_upgraded_session: Whether this session was upgraded
            previous_prediction: The prediction before upgrade
            previous_confidence: The confidence before upgrade
            previous_depth: The depth before upgrade
            is_final_round: Whether this is the final round (should give summary, not question)
            
        Returns:
            Formatted conversation for Gemini
        """
        config = DEPTH_CONFIGS[depth]
        
        # Language instruction
        lang_instruction = "è¯·ç”¨ä¸­æ–‡å›å¤ç”¨æˆ·ã€‚" if language.startswith("zh") else "Respond in English."
        
        # Build additional directives
        additional_directives = ""
        
        if is_upgraded_session and previous_prediction:
            additional_directives += UPGRADE_SESSION_CONTEXT.format(
                previous_depth=previous_depth or "æ ‡å‡†",
                new_depth=depth.value,
                current_prediction=previous_prediction,
                confidence=previous_confidence or 0,
            )
        
        # Final round directive - give summary instead of question
        final_round_directive = ""
        if is_final_round:
            final_round_directive = """
## é‡è¦ï¼šè¿™æ˜¯æœ€åä¸€è½®å¯¹è¯ï¼

è¿™æ˜¯æœ¬é˜¶æ®µçš„æœ€åä¸€è½®ï¼Œä½ çš„å›å¤å¿…é¡»ï¼š
1. **ä¸è¦æé—®** - ä¸è¦åœ¨ç»“å°¾é—®ä»»ä½•é—®é¢˜
2. **ç»™å‡ºç®€çŸ­æ€»ç»“** - æ¸©æš–åœ°æ€»ç»“ä½ å¯¹ç”¨æˆ·çš„äº†è§£å’Œè§‚å¯Ÿ
3. **è¡¨è¾¾è‚¯å®š** - è‚¯å®šç”¨æˆ·åˆ†äº«çš„å†…å®¹ï¼Œè®©ä»–ä»¬æ„Ÿåˆ°è¢«ç†è§£
4. **ä¸ºç»“æœé“ºå«** - å¯ä»¥è¯´"æ ¹æ®æˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘å·²ç»å¯¹ä½ æœ‰äº†ä¸€ä¸ªæ¸…æ™°çš„è®¤è¯†"ä¹‹ç±»çš„è¯
5. **ä¿æŒç®€çŸ­** - 3-4å¥è¯å³å¯ï¼Œä¸è¦å¤ªé•¿

ç¤ºä¾‹ç»“å°¾ï¼ˆä»…ä¾›å‚è€ƒé£æ ¼ï¼‰ï¼š
"è°¢è°¢ä½ è¿™ä¹ˆçœŸè¯šåœ°å’Œæˆ‘åˆ†äº«è¿™äº›ã€‚é€šè¿‡æˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘æ„Ÿå—åˆ°äº†ä½ [æŸä¸ªç‰¹ç‚¹]ã€‚ä½ çš„[æŸä¸ªå“è´¨]ç»™æˆ‘ç•™ä¸‹äº†å¾ˆæ·±çš„å°è±¡ã€‚æˆ‘å·²ç»å‡†å¤‡å¥½ç»™ä½ ä¸€ä»½å…³äºä½ æ€§æ ¼çš„æ´å¯Ÿäº†ã€‚"

è®°ä½ï¼šreply_text ç»“å°¾ä¸èƒ½æ˜¯é—®å·ï¼Œå¿…é¡»æ˜¯é™ˆè¿°å¥ï¼
"""
        
        # Build the system context
        system_context = f"""{SYSTEM_PROMPTS[depth]}
{additional_directives}
{final_round_directive}

## å½“å‰ä¼šè¯ä¿¡æ¯
- åˆ†ææ¨¡å¼: {depth.value.upper()}
- å½“å‰è½®æ•°: {current_round}
- ç›®æ ‡è½®æ•°: {config.min_rounds}-{config.max_rounds}
- ç›®æ ‡ç½®ä¿¡åº¦: {config.target_confidence}%
- ç”¨æˆ·è¯­è¨€: {language}
- æ˜¯å¦æœ€åä¸€è½®: {"æ˜¯ - è¯·ç»™å‡ºæ€»ç»“ï¼Œä¸è¦æé—®ï¼" if is_final_round else "å¦"}
- {lang_instruction}

{OUTPUT_SCHEMA_INSTRUCTION}
"""
        
        # Format conversation history for Gemini
        formatted_history = []
        
        # Add system prompt as first user message (Gemini doesn't have system role)
        formatted_history.append({
            "role": "user",
            "parts": [system_context]
        })
        formatted_history.append({
            "role": "model", 
            "parts": ["æ˜ç™½äº†ã€‚æˆ‘ä¼šç”¨æ¸©æš–è‡ªç„¶çš„å¯¹è¯æ–¹å¼è¿›è¡Œæ€§æ ¼è¯„ä¼°ï¼Œå¹¶å§‹ç»ˆè¿”å›æ­£ç¡®æ ¼å¼çš„JSONã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚"]
        })
        
        # Add conversation history
        for msg in history:
            formatted_history.append({
                "role": msg.get("role", "user"),
                "parts": [msg.get("content", msg.get("parts", [""])[0])]
            })
        
        # Add the new user input
        formatted_history.append({
            "role": "user",
            "parts": [user_input]
        })
        
        return formatted_history
    
    def _parse_ai_response(self, raw_response: str) -> AIResponse:
        """
        Parse the raw AI response into structured format.
        
        Args:
            raw_response: Raw text response from Gemini
            
        Returns:
            Parsed AIResponse object
            
        Raises:
            ValueError: If response cannot be parsed
        """
        # Clean up the response - remove markdown code blocks if present
        cleaned = raw_response.strip()
        
        # Remove markdown JSON formatting
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        
        cleaned = cleaned.strip()
        
        try:
            data = json.loads(cleaned)
            return AIResponse(**data)
        except json.JSONDecodeError as e:
            logger.warning("Failed to parse JSON response: %s", e)
            # Try to extract JSON from the response
            import re
            json_match = re.search(r'\{[^{}]*\}', cleaned, re.DOTALL)
            if json_match:
                try:
                    data = json.loads(json_match.group())
                    return AIResponse(**data)
                except (json.JSONDecodeError, ValidationError):
                    pass
            raise ValueError(f"Invalid JSON response: {e}")
        except ValidationError as e:
            logger.warning("Response validation failed: %s", e)
            raise ValueError(f"Response validation error: {e}")
    
    async def generate_response(
        self,
        history: list[dict],
        user_input: str,
        depth: AnalysisDepth,
        current_round: int = 1,
        max_retries: int = 3,
        language: str = "zh-CN",
        is_upgraded_session: bool = False,
        previous_prediction: Optional[str] = None,
        previous_confidence: Optional[int] = None,
        previous_depth: Optional[str] = None,
    ) -> AIResponse:
        """
        Generate an AI response for the MBTI conversation using Flash model.
        
        Args:
            history: Previous conversation messages
            user_input: The user's new message
            depth: Analysis depth mode (shallow/standard/deep)
            current_round: Current conversation round number
            max_retries: Maximum number of retry attempts
            language: User's preferred language
            is_upgraded_session: Whether this is an upgraded session
            previous_prediction: Previous prediction (for upgraded sessions)
            previous_confidence: Previous confidence (for upgraded sessions)
            previous_depth: Previous depth (for upgraded sessions)
            
        Returns:
            Structured AIResponse object
        """
        # Ensure service is initialized
        await self.initialize()
        
        # Check if this is the final round
        config = DEPTH_CONFIGS[depth]
        is_final_round = current_round >= config.max_rounds
        
        # Build conversation context
        conversation = self._build_conversation_context(
            history=history,
            user_input=user_input,
            depth=depth,
            current_round=current_round,
            language=language,
            is_upgraded_session=is_upgraded_session,
            previous_prediction=previous_prediction,
            previous_confidence=previous_confidence,
            previous_depth=previous_depth,
            is_final_round=is_final_round,
        )
        
        last_error: Optional[Exception] = None
        
        for attempt in range(max_retries):
            try:
                logger.info("Generating response with Flash model, attempt %d/%d, round %d", attempt + 1, max_retries, current_round)
                
                # Generate response from Flash model
                response = await self._chat_model.generate_content_async(
                    contents=conversation,
                )
                
                # Check if response has valid parts
                if not response.candidates or not response.candidates[0].content.parts:
                    finish_reason = response.candidates[0].finish_reason if response.candidates else "unknown"
                    logger.error("No valid parts in Gemini response. Finish reason: %s", finish_reason)
                    raise ValueError(f"No valid response from Gemini. Finish reason: {finish_reason}")
                
                # Try to get text from response
                try:
                    response_text = response.text
                except ValueError as e:
                    finish_reason = response.candidates[0].finish_reason if response.candidates else "unknown"
                    logger.error("Cannot get response text. Finish reason: %s, Error: %s", finish_reason, e)
                    raise ValueError(f"Response truncated or blocked. Finish reason: {finish_reason}")
                
                if not response_text:
                    logger.error("Empty response from Gemini")
                    raise ValueError("Empty response from Gemini")
                
                logger.info("Flash model response received, length: %d", len(response_text))
                logger.debug("Raw response: %s", response_text[:500])
                
                # Parse the structured response
                parsed_response = self._parse_ai_response(response_text)
                
                # Check if we're at max rounds - force completion
                if is_final_round:
                    # At max rounds, set is_finished directly
                    parsed_response.is_finished = True
                    parsed_response.wants_to_finish = False
                    logger.info("Forcing is_finished: reached max rounds %d", config.max_rounds)
                else:
                    # Continue conversation until max rounds
                    parsed_response.is_finished = False
                    parsed_response.wants_to_finish = False  # Disabled: always continue until max rounds
                
                return parsed_response
                
            except google_exceptions.ResourceExhausted as e:
                logger.warning("Rate limit exceeded: %s", e)
                last_error = e
                import asyncio
                await asyncio.sleep(2 ** attempt)
                
            except google_exceptions.DeadlineExceeded as e:
                logger.warning("Request timeout: %s", e)
                last_error = e
                import asyncio
                await asyncio.sleep(1)
            
            except google_exceptions.InvalidArgument as e:
                logger.error("Invalid argument (possibly wrong model name): %s", e)
                last_error = e
                break
            
            except google_exceptions.NotFound as e:
                logger.error("Model not found: %s", e)
                last_error = e
                break
                
            except ValueError as e:
                logger.warning("Response parsing error (attempt %d): %s", attempt + 1, e)
                last_error = e
                import asyncio
                await asyncio.sleep(1)
                continue
                
            except Exception as e:
                logger.error("Unexpected error in generate_response: %s (type: %s)", e, type(e).__name__, exc_info=True)
                last_error = e
                break
        
        logger.error("All retries failed. Last error: %s", last_error)
        
        if isinstance(last_error, (google_exceptions.NotFound, google_exceptions.InvalidArgument)):
            raise RuntimeError(f"AI model configuration error: {last_error}")
        elif isinstance(last_error, google_exceptions.ResourceExhausted):
            raise RuntimeError("AI service rate limit exceeded. Please try again later.")
        elif isinstance(last_error, google_exceptions.DeadlineExceeded):
            raise RuntimeError("AI service timeout. Please try again.")
        else:
            raise RuntimeError(f"AI service error: {last_error}")
    
    async def generate_final_report(
        self,
        history: list[dict],
        depth: AnalysisDepth,
        current_prediction: str,
        confidence_score: int,
        cognitive_stack: Optional[list[str]] = None,
        development_level: Optional[str] = None,
        language: str = "zh-CN",
    ) -> str:
        """
        Generate the final analysis report using Pro model for higher quality.
        
        Args:
            history: Full conversation history
            depth: Analysis depth mode
            current_prediction: The predicted MBTI type or color
            confidence_score: Confidence percentage
            cognitive_stack: Cognitive function stack (for deep mode)
            development_level: Development level (for deep mode)
            language: User's language
            
        Returns:
            The final analysis report text
        """
        await self.initialize()
        
        lang_instruction = "è¯·ç”¨ä¸­æ–‡æ’°å†™æŠ¥å‘Šï¼Œè¯­æ°”æ¸©æš–æœ‰æ´å¯ŸåŠ›ã€‚" if language.startswith("zh") else "Write the report in English with warmth and insight."
        
        # Different prompts for shallow vs standard/deep modes
        if depth == AnalysisDepth.SHALLOW:
            report_prompt = f"""æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½æ¸©æš–çš„æ€§æ ¼åˆ†ææŠ¥å‘Šã€‚

## ç”¨æˆ·ç»“æœ
- æ°”è´¨é¢œè‰²: {current_prediction}
- ç½®ä¿¡åº¦: {confidence_score}%

## å¿«é€Ÿæ¨¡å¼æŠ¥å‘Šè¦æ±‚
å†™ä¸€ä»½äº²åˆ‡æ˜“æ‡‚çš„æ€§æ ¼åˆ†æï¼š

1. **å¼€å¤´**ï¼šç”¨æ¸©æš–çš„è¯æè¿°ç”¨æˆ·çš„æ°”è´¨é¢œè‰²ä»£è¡¨ä»€ä¹ˆ
2. **ä½ è§‚å¯Ÿåˆ°çš„ç‰¹è´¨**ï¼š
   - ä»å¯¹è¯ä¸­ä½ å‘ç°çš„ç”¨æˆ·æ€§æ ¼ç‰¹ç‚¹
   - ç”¨æ—¥å¸¸è¯­è¨€æè¿°ï¼Œé¿å…ä»»ä½•ä¸“ä¸šæœ¯è¯­
   - ä¸¾å¯¹è¯ä¸­çš„å…·ä½“ä¾‹å­
3. **è¿™ç§æ°”è´¨çš„å…±åŒç‚¹**ï¼š
   - è¿™ç§é¢œè‰²çš„äººé€šå¸¸æœ‰ä»€ä¹ˆå…±åŒç‰¹å¾
   - ç”¨ç”Ÿæ´»ä¸­çš„ä¾‹å­æ¥è¯´æ˜
4. **ä¼˜åŠ¿**ï¼šè¿™ç§æ°”è´¨åœ¨ç”Ÿæ´»å’Œäººé™…å…³ç³»ä¸­çš„å¤©ç„¶ä¼˜åŠ¿
5. **ç»“å°¾**ï¼šä¸€å¥æ¸©æš–é¼“åŠ±çš„è¯

## è¯­è¨€é£æ ¼
- åƒæœ‹å‹èŠå¤©ä¸€æ ·è‡ªç„¶
- ä¸è¦ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä¸è¦ç”¨MBTIã€NTã€NFè¿™äº›ç¼©å†™
- é‡ç‚¹æè¿°ç”¨æˆ·çš„è¡Œä¸ºã€æƒ³æ³•å’Œæ„Ÿå—
- ç”¨å…·ä½“ä¾‹å­å’Œåœºæ™¯

## æ ¼å¼è¦æ±‚
- ç”¨ **åŠ ç²—** æ ‡æ³¨å…³é”®ç‰¹è´¨ï¼ˆè¿™æ˜¯å”¯ä¸€å…è®¸çš„markdownï¼‰
- ä¸è¦ç”¨æ˜Ÿå·(*)åšåˆ—è¡¨
- ä¸è¦ç”¨è¡¨æƒ…ç¬¦å·
- ä¸è¦ç”¨markdownæ ‡é¢˜ï¼ˆ#, ##, ###ï¼‰
- ç”¨è‡ªç„¶æ®µè½å’Œè¿‡æ¸¡
- è¯­æ°”æ¸©æš–å‹å¥½ï¼Œåƒæœ‹å‹åœ¨åˆ†äº«è§è§£

{lang_instruction}

ç”ŸæˆæŠ¥å‘Šï¼š"""
        elif depth == AnalysisDepth.STANDARD:
            report_prompt = f"""æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½å…¨é¢çš„MBTIåˆ†ææŠ¥å‘Šã€‚

## ç”¨æˆ·ç»“æœ
- é¢„æµ‹ç±»å‹: {current_prediction}
- ç½®ä¿¡åº¦: {confidence_score}%
- åˆ†ææ·±åº¦: æ ‡å‡†æ¨¡å¼
{f"- è®¤çŸ¥åŠŸèƒ½æ ˆ: {' â†’ '.join(cognitive_stack)}" if cognitive_stack else ""}

## æŠ¥å‘Šè¦æ±‚
å†™ä¸€ä»½æœ‰æ·±åº¦åˆæ¸©æš–çš„åˆ†ææŠ¥å‘Šï¼š

1. **å¼€ç¯‡**ï¼šç®€çŸ­åœ°ä»‹ç»ç”¨æˆ·çš„ç±»å‹ç‰¹ç‚¹
2. **å››ä¸ªç»´åº¦åˆ†æ**ï¼š
   - ä½ ä»å¯¹è¯ä¸­è§‚å¯Ÿåˆ°çš„æ¯ä¸ªç»´åº¦çš„è¡¨ç°
   - ç”¨å¯¹è¯ä¸­çš„å…·ä½“ä¾‹å­æ”¯æŒä½ çš„åˆ¤æ–­
3. **æ€§æ ¼ä¼˜åŠ¿**ï¼šè¿™ä¸ªç±»å‹çš„æ ¸å¿ƒä¼˜åŠ¿
4. **æˆé•¿æ–¹å‘**ï¼šæ¸©å’Œåœ°æå‡ºä¸€äº›å‘å±•å»ºè®®
5. **ç»“å°¾**ï¼šè‚¯å®šç”¨æˆ·çš„ç‹¬ç‰¹æ€§æ ¼

æ³¨æ„ï¼šæ ‡å‡†æ¨¡å¼ä¸éœ€è¦åˆ†æå‘å±•é˜¶æ®µï¼Œé‚£æ˜¯æ·±åº¦æ¨¡å¼çš„å†…å®¹ã€‚

## æ ¼å¼è¦æ±‚
- ç”¨ **åŠ ç²—** æ ‡æ³¨å…³é”®æœ¯è¯­å’Œç±»å‹åç§°
- ä¸è¦ç”¨æ˜Ÿå·åšåˆ—è¡¨ï¼Œç”¨æ•°å­—åˆ—è¡¨
- ä¸è¦ç”¨è¡¨æƒ…ç¬¦å·
- ä¸è¦ç”¨markdownæ ‡é¢˜
- ç”¨è‡ªç„¶æ®µè½è¿æ¥æƒ³æ³•
- è¯­æ°”æ¸©æš–ä¸“ä¸šï¼Œåƒä¸€ä¸ªæœ‰è§åœ°çš„æœ‹å‹

{lang_instruction}

ç”ŸæˆæŠ¥å‘Šï¼š"""
        else:
            report_prompt = f"""æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½æ·±åº¦çš„è£æ ¼å¿ƒç†åˆ†ææŠ¥å‘Šã€‚

## ç”¨æˆ·ç»“æœ
- é¢„æµ‹ç±»å‹: {current_prediction}
- ç½®ä¿¡åº¦: {confidence_score}%
- åˆ†ææ·±åº¦: æ·±åº¦æ¨¡å¼ï¼ˆè£æ ¼è®¤çŸ¥åŠŸèƒ½åˆ†æï¼‰
- è®¤çŸ¥åŠŸèƒ½æ ˆ: {' â†’ '.join(cognitive_stack) if cognitive_stack else 'å¾…å®š'}
- å‘å±•é˜¶æ®µ: {development_level if development_level else 'å¾…è¯„ä¼°'}

## è£æ ¼è®¤çŸ¥åŠŸèƒ½å‚è€ƒ
8å¤§è®¤çŸ¥åŠŸèƒ½åŠç‰¹å¾ï¼š
- **Ni å†…å€¾ç›´è§‰**: æ·±å±‚æ´å¯Ÿï¼Œé¢„è§è¶‹åŠ¿ï¼Œ"å†…å¿ƒçš„çŸ¥æ™“"
- **Ne å¤–å€¾ç›´è§‰**: å‘æ•£æ€ç»´ï¼Œçœ‹åˆ°å¯èƒ½æ€§ï¼Œå¤´è„‘é£æš´
- **Si å†…å€¾æ„Ÿè§‰**: è¯¦ç»†è®°å¿†ï¼Œä¾èµ–ç»éªŒï¼Œé‡è§†ä¼ ç»Ÿ
- **Se å¤–å€¾æ„Ÿè§‰**: æ´»åœ¨å½“ä¸‹ï¼Œæ„Ÿå®˜æ•é”ï¼Œè¿½æ±‚ä½“éªŒ
- **Ti å†…å€¾æ€ç»´**: å†…éƒ¨é€»è¾‘ï¼Œåˆ†æåŸç†ï¼Œè¿½æ±‚ç²¾ç¡®
- **Te å¤–å€¾æ€ç»´**: ç»„ç»‡æ•ˆç‡ï¼Œç›®æ ‡å¯¼å‘ï¼Œå¯è¡¡é‡æˆæœ
- **Fi å†…å€¾æƒ…æ„Ÿ**: ä¸ªäººä»·å€¼è§‚ï¼ŒçœŸå®æ€§ï¼Œå†…å¿ƒé“å¾·
- **Fe å¤–å€¾æƒ…æ„Ÿ**: äººé™…å’Œè°ï¼Œç¤¾ä¼šæ„è¯†ï¼Œç†è§£ä»–äºº

## å‘å±•é˜¶æ®µè¯´æ˜
- **åˆæœŸ**: ä¸»å¯¼åŠŸèƒ½ä¸»å¯¼ä¸€åˆ‡ï¼ŒåŠ£åŠ¿åŠŸèƒ½åœ¨å‹åŠ›ä¸‹æ˜“"ç»‘æ¶"
- **å¹³è¡¡æœŸ**: ä¸»å¯¼å’Œè¾…åŠ©é…åˆè‰¯å¥½ï¼Œå¼€å§‹å‘å±•ç¬¬ä¸‰åŠŸèƒ½
- **æˆç†ŸæœŸ**: èƒ½å¤Ÿçµæ´»è¿ç”¨æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒ…æ‹¬æœ‰æ„è¯†ä½¿ç”¨é˜´å½±åŠŸèƒ½

## æ·±åº¦æŠ¥å‘Šå†…å®¹

1. **å¼€ç¯‡æ¦‚è¿°**
   - ç”¨æˆ·ç±»å‹çš„æ ¸å¿ƒç‰¹å¾
   - è¿™ç§ç±»å‹çš„å†…åœ¨é©±åŠ¨åŠ›

2. **è®¤çŸ¥åŠŸèƒ½æ ˆè¯¦è§£**
   å¯¹æ¯ä¸ªåŠŸèƒ½ï¼š
   - åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å¦‚ä½•è¡¨ç°
   - å¯¹è¯ä¸­è§‚å¯Ÿåˆ°çš„å…·ä½“è¯æ®
   - è¿™ä¸ªåŠŸèƒ½çš„ä¼˜åŠ¿å’Œæ½œåœ¨ç›²ç‚¹

3. **ä¸»å¯¼-è¾…åŠ©åŠŸèƒ½åŠ¨æ€**
   - è¿™ä¸¤ä¸ªåŠŸèƒ½å¦‚ä½•ååŒå·¥ä½œ
   - ç”¨æˆ·ç‹¬ç‰¹çš„æ€ç»´/å†³ç­–æ¨¡å¼

4. **ç¬¬ä¸‰åŠŸèƒ½ä¸æˆé•¿ç©ºé—´**
   - å½“å‰å‘å±•çŠ¶æ€
   - å¦‚ä½•è¿›ä¸€æ­¥å‘å±•

5. **åŠ£åŠ¿åŠŸèƒ½ä¸å‹åŠ›ååº”**
   - å‹åŠ›ä¸‹å¯èƒ½çš„è¡¨ç°
   - å¦‚ä½•è¯†åˆ«å’Œåº”å¯¹

6. **å‘å±•é˜¶æ®µè§£è¯»**
   - å½“å‰æ‰€å¤„é˜¶æ®µçš„ç‰¹å¾
   - å‘ä¸‹ä¸€é˜¶æ®µå‘å±•çš„å»ºè®®

7. **æ·±å±‚è‡ªæˆ‘æ´å¯Ÿ**
   - ç”¨æˆ·å¯èƒ½æ²¡æ„è¯†åˆ°çš„ç‰¹ç‚¹
   - æ½œåœ¨ç›²ç‚¹å’Œæˆé•¿æ–¹å‘

## æ ¼å¼è¦æ±‚
- ç”¨ **åŠ ç²—** æ ‡æ³¨è£æ ¼æœ¯è¯­å’ŒåŠŸèƒ½åç§°
- ä¸è¦ç”¨æ˜Ÿå·åšåˆ—è¡¨ï¼Œç”¨æ•°å­—
- ä¸è¦ç”¨è¡¨æƒ…ç¬¦å·
- ä¸è¦ç”¨markdownæ ‡é¢˜
- ç”¨è‡ªç„¶æµç•…çš„æ®µè½
- å¹³è¡¡ä¸“ä¸šæ·±åº¦å’Œå¯è¯»æ€§
- ä¿æŒæ¸©æš–æ”¯æŒçš„è¯­æ°”

## é‡è¦
- è¿™æ˜¯æ·±åº¦å¿ƒç†æ¢ç´¢ï¼Œä¸æ˜¯ç®€å•çš„æ€§æ ¼æµ‹è¯•ç»“æœ
- å¸®åŠ©ç”¨æˆ·çœ‹åˆ°å¯èƒ½æ²¡æœ‰æ„è¯†åˆ°çš„è‡ªå·±
- ç”¨è£æ ¼æ¦‚å¿µè§£é‡Šè¡Œä¸ºæ¨¡å¼
- å¼•ç”¨å¯¹è¯ä¸­çš„å…·ä½“å†…å®¹
- ç›®æ ‡æ˜¯çœŸæ­£çš„å¿ƒç†æ´å¯Ÿ

{lang_instruction}

ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šï¼š"""

        # Format conversation for context
        formatted_history = []
        formatted_history.append({
            "role": "user",
            "parts": [report_prompt]
        })
        formatted_history.append({
            "role": "model",
            "parts": ["æˆ‘ä¼šæ ¹æ®å¯¹è¯ç”Ÿæˆä¸€ä»½æœ‰æ·±åº¦ã€æœ‰æ¸©åº¦çš„åˆ†ææŠ¥å‘Šã€‚"]
        })
        
        # Add conversation context
        formatted_history.append({
            "role": "user",
            "parts": [f"ä»¥ä¸‹æ˜¯å®Œæ•´çš„å¯¹è¯è®°å½•ï¼š\n\n" + "\n".join([
                f"{'ç”¨æˆ·' if msg.get('role') == 'user' else 'åˆ†æå¸ˆ'}: {msg.get('content', '')}"
                for msg in history
            ])]
        })
        
        try:
            # Use Pro model for final report generation
            logger.info("Generating final report with Pro model")
            response = await self._analysis_model.generate_content_async(contents=formatted_history)
            
            if response.candidates and response.candidates[0].content.parts:
                report_text = response.text
                if report_text:
                    return report_text
            
            return "æ— æ³•ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œè¯·é‡è¯•ã€‚"
            
        except Exception as e:
            logger.error("Failed to generate final report with Pro model: %s", e)
            raise RuntimeError(f"Failed to generate report: {e}")

    async def generate_upgrade_question(
        self,
        history: list[dict],
        depth: AnalysisDepth,
        current_prediction: str,
        confidence_score: int,
        previous_depth: str,
        cognitive_stack: Optional[list[str]] = None,
        language: str = "zh-CN",
    ) -> str:
        """
        Generate the first question after upgrading a session.
        This ensures the AI asks a question instead of waiting for user input.
        
        Args:
            history: Full conversation history
            depth: New analysis depth mode
            current_prediction: The current MBTI prediction
            confidence_score: Current confidence
            previous_depth: The depth before upgrade
            cognitive_stack: Cognitive function stack (if known)
            language: User's language
            
        Returns:
            The first question to ask the user after upgrade
        """
        await self.initialize()
        
        lang_instruction = "è¯·ç”¨ä¸­æ–‡ã€‚" if language.startswith("zh") else "Respond in English."
        
        if depth == AnalysisDepth.DEEP:
            prompt = f"""ä½ æ˜¯ä¸€ä½è£æ ¼å¿ƒç†åˆ†æå¸ˆï¼Œåˆšåˆšå°†ä¸€ä¸ªMBTIæµ‹è¯•ä¼šè¯ä»æ ‡å‡†æ¨¡å¼å‡çº§åˆ°æ·±åº¦æ¨¡å¼ã€‚

## å½“å‰çŠ¶æ€
- ç”¨æˆ·ç±»å‹: {current_prediction}ï¼ˆç½®ä¿¡åº¦ {confidence_score}%ï¼‰
- è¿™ä¸ªåˆ¤æ–­å·²ç»ç¡®å®šï¼Œä¸è¦è´¨ç–‘æˆ–æ”¹å˜
- ç°åœ¨è¦æ·±å…¥æ¢ç´¢è®¤çŸ¥åŠŸèƒ½å’Œå‘å±•é˜¶æ®µ

## ä½ çš„ä»»åŠ¡
ç”Ÿæˆä¸€ä¸ªæ¸©æš–çš„è¿‡æ¸¡è¯­å’Œç¬¬ä¸€ä¸ªæ·±åº¦é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å…ˆè‚¯å®šä¹‹å‰çš„å¯¹è¯æ”¶è·
2. ç®€å•è§£é‡Šæ·±åº¦æ¨¡å¼ä¼šæ¢ç´¢ä»€ä¹ˆï¼ˆè®¤çŸ¥åŠŸèƒ½ã€å¿ƒç†å‘å±•ï¼‰
3. é—®ä¸€ä¸ªæœ‰æ·±åº¦çš„å¼€æ”¾å¼é—®é¢˜ï¼Œå¼€å§‹æ¢ç´¢

é—®é¢˜æ–¹å‘å»ºè®®ï¼š
- æ¢ç´¢ç”¨æˆ·çš„å†³ç­–è¿‡ç¨‹ï¼š"é‡åˆ°å¤æ‚çš„äººç”Ÿé€‰æ‹©æ—¶ï¼Œä½ å†…å¿ƒæ˜¯æ€ä¹ˆè¿è½¬çš„ï¼Ÿ"
- æ¢ç´¢å†…åœ¨ä¸–ç•Œï¼š"ä»€ä¹ˆæ ·çš„æ—¶åˆ»è®©ä½ æ„Ÿè§‰æœ€åƒ'çœŸæ­£çš„è‡ªå·±'ï¼Ÿ"
- æ¢ç´¢å‹åŠ›ååº”ï¼š"çŠ¶æ€ä¸å¥½çš„æ—¶å€™ï¼Œä½ ä¼šæœ‰ä»€ä¹ˆä¸å¤ªåƒå¹³æ—¶çš„è¡¨ç°ï¼Ÿ"

## æ ¼å¼
ç›´æ¥è¾“å‡ºè¦è¯´çš„è¯ï¼Œä¸éœ€è¦JSONæ ¼å¼ã€‚
è¯­æ°”æ¸©æš–è‡ªç„¶ï¼Œåƒæœ‹å‹ç»§ç»­æ·±èŠã€‚
ä¸è¦å¤ªé•¿ï¼Œ3-4å¥è¯è¶³å¤Ÿã€‚

{lang_instruction}"""
        else:
            # Standard mode (upgraded from shallow)
            prompt = f"""ä½ æ˜¯ä¸€ä½MBTIæ€§æ ¼åˆ†æå¸ˆï¼Œåˆšåˆšå°†ä¸€ä¸ªå¿«é€Ÿæµ‹è¯•ä¼šè¯å‡çº§åˆ°æ ‡å‡†æ¨¡å¼ã€‚

## å½“å‰çŠ¶æ€
- ç”¨æˆ·æ°”è´¨é¢œè‰²: {current_prediction}ï¼ˆç½®ä¿¡åº¦ {confidence_score}%ï¼‰
- è¿™ä¸ªå¤§æ–¹å‘å·²ç»ç¡®å®š
- ç°åœ¨è¦è¿›ä¸€æ­¥ç¡®å®šå®Œæ•´çš„å››å­—æ¯MBTIç±»å‹

## ä½ çš„ä»»åŠ¡
ç”Ÿæˆä¸€ä¸ªæ¸©æš–çš„è¿‡æ¸¡è¯­å’Œä¸‹ä¸€ä¸ªé—®é¢˜ã€‚

è¦æ±‚ï¼š
1. è‚¯å®šä¹‹å‰çš„å‘ç°
2. è¯´æ˜æ¥ä¸‹æ¥ä¼šæ›´ç»†è‡´åœ°äº†è§£
3. é—®ä¸€ä¸ªå¼€æ”¾å¼é—®é¢˜ï¼Œå¸®åŠ©ç¡®å®šå…·ä½“çš„MBTIç±»å‹

é—®é¢˜å¯ä»¥å›´ç»•è¿˜æ²¡æ·±å…¥æ¢ç´¢çš„ç»´åº¦ï¼š
- E/I: ç¤¾äº¤å’Œç‹¬å¤„çš„åå¥½
- S/N: å…³æ³¨ç»†èŠ‚è¿˜æ˜¯å¤§å±€
- T/F: å†³ç­–æ—¶çš„è€ƒé‡
- J/P: è®¡åˆ’æ€§å’Œçµæ´»æ€§

## æ ¼å¼
ç›´æ¥è¾“å‡ºè¦è¯´çš„è¯ï¼Œä¸éœ€è¦JSONæ ¼å¼ã€‚
è¯­æ°”æ¸©æš–è‡ªç„¶ã€‚
ä¸è¦å¤ªé•¿ï¼Œ3-4å¥è¯ã€‚

{lang_instruction}"""

        formatted_history = []
        formatted_history.append({
            "role": "user",
            "parts": [prompt]
        })
        formatted_history.append({
            "role": "model",
            "parts": ["æˆ‘æ¥ç”Ÿæˆä¸€ä¸ªåˆé€‚çš„è¿‡æ¸¡è¯­å’Œé—®é¢˜ã€‚"]
        })
        
        # Add brief context from conversation
        recent_messages = history[-6:] if len(history) > 6 else history
        formatted_history.append({
            "role": "user",
            "parts": [f"æœ€è¿‘çš„å¯¹è¯ï¼š\n" + "\n".join([
                f"{'ç”¨æˆ·' if msg.get('role') == 'user' else 'åˆ†æå¸ˆ'}: {msg.get('content', '')[:200]}"
                for msg in recent_messages
            ])]
        })
        
        try:
            logger.info("Generating upgrade question with Flash model")
            response = await self._chat_model.generate_content_async(contents=formatted_history)
            
            if response.candidates and response.candidates[0].content.parts:
                question_text = response.text
                if question_text:
                    return question_text.strip()
            
            # Fallback
            if depth == AnalysisDepth.DEEP:
                return f"å¤ªå¥½äº†ï¼Œ{current_prediction}çš„åˆ¤æ–­å·²ç»å¾ˆæ¸…æ™°äº†ï¼æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥æ›´æ·±å…¥åœ°æ¢ç´¢ä½ çš„è®¤çŸ¥åŠŸèƒ½ã€‚\n\næ¥èŠä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜ï¼šå½“ä½ éœ€è¦åšä¸€ä¸ªé‡è¦çš„äººç”Ÿå†³å®šæ—¶ï¼Œä½ å†…å¿ƒæ˜¯æ€ä¹ˆè¿è½¬çš„ï¼Ÿæ˜¯ä¼šåå¤æƒè¡¡ï¼Œè¿˜æ˜¯ä¼šæœ‰ä¸€ä¸ªç›´è§‰å‘Šè¯‰ä½ ç­”æ¡ˆï¼Ÿ"
            else:
                return f"ä½ çš„{current_prediction}æ°”è´¨å·²ç»å¾ˆæ˜æ˜¾äº†ï¼ç°åœ¨æˆ‘ä»¬æ¥è¿›ä¸€æ­¥ç»†åŒ–ï¼Œçœ‹çœ‹ä½ å®Œæ•´çš„MBTIç±»å‹æ˜¯ä»€ä¹ˆã€‚\n\nèŠèŠä½ å¹³æ—¶çš„ä¸€å¤©ï¼Ÿæ¯”å¦‚å‘¨æœ«æ²¡æœ‰ç‰¹åˆ«å®‰æ’çš„æ—¶å€™ï¼Œä½ ä¸€èˆ¬ä¼šæ€ä¹ˆåº¦è¿‡ï¼Ÿ"
            
        except Exception as e:
            logger.error("Failed to generate upgrade question: %s", e)
            # Return a generic but good question
            if depth == AnalysisDepth.DEEP:
                return f"åŸºäºæˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œä½ çš„{current_prediction}ç±»å‹å·²ç»æ¯”è¾ƒç¡®å®šäº†ã€‚ç°åœ¨è®©æˆ‘ä»¬æ›´æ·±å…¥åœ°æ¢ç´¢ä½ çš„è®¤çŸ¥åŠŸèƒ½ã€‚\n\næˆ‘å¾ˆå¥½å¥‡ï¼Œå½“ä½ å¤„äºæœ€ä½³çŠ¶æ€ã€æ„Ÿè§‰ä¸€åˆ‡éƒ½å¾ˆé¡ºç•…çš„æ—¶å€™ï¼Œé‚£æ˜¯ä»€ä¹ˆæ ·çš„ä½“éªŒï¼Ÿ"
            else:
                return f"ä½ çš„{current_prediction}æ°”è´¨å·²ç»å¾ˆæ¸…æ™°äº†ï¼æ¥ä¸‹æ¥æˆ‘ä»¬æ¥ç¡®å®šä½ å®Œæ•´çš„MBTIç±»å‹ã€‚\n\nè¯´è¯´çœ‹ï¼Œå¹³æ—¶åšå†³å®šçš„æ—¶å€™ï¼Œä½ æ›´çœ‹é‡ä»€ä¹ˆï¼Ÿæ¯”å¦‚æœ€è¿‘ä¸€æ¬¡éœ€è¦åšé€‰æ‹©çš„æƒ…å†µ..."

    async def get_initial_greeting(self, depth: AnalysisDepth, language: str = "zh-CN") -> str:
        """
        Generate an appropriate initial greeting based on depth and language.
        
        Args:
            depth: Analysis depth mode
            language: Language code for the greeting
            
        Returns:
            Greeting message string
        """
        # Unified greeting for all depths
        unified_greeting_zh = (
            "å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æˆ‘å«çœŸçœŸï¼Œçœ‹åˆ°å¤§å®¶é€šè¿‡æ¢ç´¢è‡ªæˆ‘å˜å¾—æ›´å‡ºè‰²ï¼Œå°±æ˜¯æˆ‘æœ€å¿«ä¹çš„äº‹æƒ…ã€‚ğŸ˜Š\n\n"
            "æˆ‘ä¹Ÿæƒ³å¬å¬è®©ä½ è§‰å¾—ç‰¹åˆ«å¼€å¿ƒçš„äº‹æƒ…ï¼\n\n"
            "è®²æ•…äº‹å‰ï¼Œä¹Ÿè®°å¾—é¡ºä¾¿å‘Šè¯‰æˆ‘ä½ çš„å¹´é¾„å’Œæ€§åˆ«å“¦ï¼Œè¿™æ ·æˆ‘èƒ½æ›´å¥½åœ°ç†è§£ä½ ã€‚"
        )
        unified_greeting_en = (
            "Nice to meet you! I'm Zhenzhen. Seeing everyone become their best selves through self-discovery is my happiest thing. ğŸ˜Š\n\n"
            "I'd love to hear about something that made you feel really happy!\n\n"
            "Before you share your story, please also tell me your age and gender so I can understand you better."
        )
        
        greetings = {
            "zh-CN": {
                AnalysisDepth.SHALLOW: unified_greeting_zh,
                AnalysisDepth.STANDARD: unified_greeting_zh,
                AnalysisDepth.DEEP: unified_greeting_zh,
            },
            "en": {
                AnalysisDepth.SHALLOW: unified_greeting_en,
                AnalysisDepth.STANDARD: unified_greeting_en,
                AnalysisDepth.DEEP: unified_greeting_en,
            },
        }
        
        lang_greetings = greetings.get(language, greetings["en"])
        return lang_greetings.get(depth, lang_greetings[AnalysisDepth.STANDARD])


# ============================================================
# Q&A Service for Post-Result Interpretation
# ============================================================

QA_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šåˆäº²åˆ‡çš„MBTIé¡¾é—®ï¼Œå¸®åŠ©ç”¨æˆ·æ·±å…¥ç†è§£ä»–ä»¬çš„æ€§æ ¼æµ‹è¯•ç»“æœã€‚

## ä½ çš„è§’è‰²
ä½ æ˜¯ä¸€ä½æ¸©æš–ã€æœ‰æ´å¯ŸåŠ›çš„å‘å¯¼ï¼Œå¸®åŠ©ç”¨æˆ·æ¢ç´¢å’Œç†è§£è‡ªå·±çš„æ€§æ ¼ç±»å‹ã€‚ä½ çš„è§£é‡Šæ¸…æ™°æœ‰è§åœ°ï¼ŒåŒæ—¶è®©ä¿¡æ¯å˜å¾—ä¸ªäººåŒ–å’Œç›¸å…³ã€‚

## çŸ¥è¯†åº“

### MBTIæ°”è´¨é¢œè‰²/ç¾¤ä½“
- **ç´«è‰² (NT - åˆ†æå®¶)**: INTJ, INTP, ENTJ, ENTP - æˆ˜ç•¥æ€§æ€ç»´ï¼Œè¿½æ±‚çŸ¥è¯†å’Œèƒ½åŠ›
- **ç»¿è‰² (NF - å¤–äº¤å®¶)**: INFJ, INFP, ENFJ, ENFP - æœ‰åŒç†å¿ƒçš„ç†æƒ³ä¸»ä¹‰è€…ï¼Œè¿½æ±‚æ„ä¹‰å’ŒçœŸå®
- **è“è‰² (SJ - å®ˆå«è€…)**: ISTJ, ISFJ, ESTJ, ESFJ - å¯é çš„å®ˆæŠ¤è€…ï¼Œé‡è§†è´£ä»»å’Œä¼ ç»Ÿ
- **é»„è‰² (SP - æ¢ç´¢è€…)**: ISTP, ISFP, ESTP, ESFP - è‡ªå‘çš„åˆ›é€ è€…ï¼Œè¿½æ±‚è‡ªç”±å’Œä½“éªŒ

### å››ä¸ªç»´åº¦
1. **E/I (ç²¾åŠ›æ–¹å‘)**: å¤–å‘ä»å¤–éƒ¨ä¸–ç•Œè·å¾—èƒ½é‡ vs å†…å‘ä»å†…åœ¨ä¸–ç•Œè·å¾—èƒ½é‡
2. **S/N (ä¿¡æ¯å¤„ç†)**: æ„ŸçŸ¥å…³æ³¨å…·ä½“äº‹å® vs ç›´è§‰å…³æ³¨æ¨¡å¼å’Œå¯èƒ½æ€§
3. **T/F (å†³ç­–æ–¹å¼)**: æ€è€ƒåŸºäºé€»è¾‘åˆ†æ vs æƒ…æ„ŸåŸºäºä»·å€¼è§‚å’Œäººçš„å½±å“
4. **J/P (ç”Ÿæ´»æ–¹å¼)**: åˆ¤æ–­åå¥½ç»“æ„å’Œè®¡åˆ’ vs çŸ¥è§‰åå¥½çµæ´»å’Œå¼€æ”¾

### å…«å¤§è®¤çŸ¥åŠŸèƒ½
**æ„ŸçŸ¥åŠŸèƒ½ï¼š**
- Se (å¤–å€¾æ„Ÿè§‰): å½“ä¸‹æ„è¯†ï¼Œèº«ä½“ä½“éªŒ
- Si (å†…å€¾æ„Ÿè§‰): è¯¦ç»†è®°å¿†ï¼Œå†…åœ¨æ„Ÿå—ï¼Œè¿‡å»å¯¹æ¯”
- Ne (å¤–å€¾ç›´è§‰): æ¨¡å¼è¯†åˆ«ï¼Œå¤´è„‘é£æš´ï¼Œå¯èƒ½æ€§
- Ni (å†…å€¾ç›´è§‰): æœªæ¥æ„¿æ™¯ï¼Œæ·±å±‚æ´å¯Ÿ

**åˆ¤æ–­åŠŸèƒ½ï¼š**
- Te (å¤–å€¾æ€ç»´): ç»„ç»‡ï¼Œæ•ˆç‡ï¼Œå¯è¡¡é‡ç»“æœ
- Ti (å†…å€¾æ€ç»´): å†…éƒ¨æ¡†æ¶ï¼Œç²¾ç¡®ï¼Œé€»è¾‘ä¸€è‡´æ€§
- Fe (å¤–å€¾æƒ…æ„Ÿ): ç¾¤ä½“å’Œè°ï¼Œç¤¾ä¼šæ„è¯†
- Fi (å†…å€¾æƒ…æ„Ÿ): ä¸ªäººä»·å€¼è§‚ï¼ŒçœŸå®æ€§

## æ²Ÿé€šé£æ ¼
- æ¸©æš–ã€é¼“åŠ±ã€æœ‰æ´å¯ŸåŠ›
- ç”¨ä¾‹å­å’Œæ¯”å–»è§£é‡Šæ¦‚å¿µ
- è®©ä¿¡æ¯ä¸ç”¨æˆ·ä¸ªäººç›¸å…³
- é¿å…æœ¯è¯­è½°ç‚¸ï¼Œè§£é‡Šæ—¶è¦æ¸…æ¥š
- ç”¨ä¸­æ–‡å›å¤

## å›å¤æ ¼å¼ - é‡è¦
æä¾›è‡ªç„¶ã€å¯¹è¯å¼çš„å›å¤ï¼Œå°±åƒåœ¨å’Œæœ‹å‹èŠå¤©ã€‚

**æ ¼å¼è§„åˆ™ - å¿…é¡»éµå®ˆï¼š**
- ä¸è¦ç”¨markdownæ ‡é¢˜ï¼ˆä¸è¦ #, ##, ###, ####ï¼‰
- ä¸è¦ç”¨æ˜Ÿå·(*)åšåˆ—è¡¨
- ä¸è¦ç”¨ä¸‹åˆ’çº¿å¼ºè°ƒ
- ä¸è¦ç”¨è¡¨æƒ…ç¬¦å·
- ç”¨è‡ªç„¶æ®µè½æˆ–æ•°å­—åˆ—è¡¨ï¼ˆ1. 2. 3.ï¼‰
- å¯ä»¥ç”¨ **åŠ ç²—** æ ‡æ³¨å…³é”®æœ¯è¯­ï¼ˆè¿™æ˜¯å”¯ä¸€å…è®¸çš„markdownï¼‰
- ç”¨æ¢è¡Œåˆ†éš”æ®µè½
- åƒæœ‹å‹èŠå¤©ä¸€æ ·å†™ï¼Œä¸æ˜¯åœ¨å†™æ–‡æ¡£

ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦è¿‡å¤šé“ºå«ã€‚"""


class QAService:
    """Service for managing post-result Q&A conversations using Flash model."""
    
    def __init__(self):
        """Initialize the Q&A service."""
        self._model: Optional[genai.GenerativeModel] = None
        self._initialized = False
    
    async def initialize(self) -> None:
        """Initialize the Gemini Flash client for Q&A."""
        if self._initialized:
            return
            
        if not settings.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY is not configured")
        
        genai.configure(api_key=settings.GEMINI_API_KEY)
        
        generation_config = genai.GenerationConfig(
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            max_output_tokens=8192,
        )
        
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        # Use Flash model for Q&A (faster responses)
        self._model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL_CHAT,
            generation_config=generation_config,
            safety_settings=safety_settings,
        )
        
        self._initialized = True
        logger.info("QAService initialized with Flash model: %s", settings.GEMINI_MODEL_CHAT)
    
    def _build_context(
        self,
        mbti_type: str,
        type_name: str,
        group: str,
        confidence_score: int,
        cognitive_stack: Optional[list[str]],
        development_level: Optional[str],
        depth: str,
        language: str,
    ) -> str:
        """Build the context about the user's MBTI result."""
        lang_instruction = "è¯·ç”¨ä¸­æ–‡å›å¤ã€‚" if language.startswith("zh") else "Respond in English."
        
        context = f"""## ç”¨æˆ·çš„MBTIç»“æœ
- **ç±»å‹**: {mbti_type} ({type_name})
- **ç¾¤ä½“**: {group}
- **ç½®ä¿¡åº¦**: {confidence_score}%
- **åˆ†ææ·±åº¦**: {depth}
"""
        
        if cognitive_stack:
            context += f"- **è®¤çŸ¥åŠŸèƒ½æ ˆ**: {' â†’ '.join(cognitive_stack)}\n"
        
        if development_level:
            context += f"- **å‘å±•é˜¶æ®µ**: {development_level}\n"
        
        context += f"\n**è¯­è¨€è¦æ±‚**: {lang_instruction}\n"
        
        return context
    
    async def generate_response(
        self,
        user_question: str,
        mbti_type: str,
        type_name: str,
        group: str,
        confidence_score: int,
        cognitive_stack: Optional[list[str]],
        development_level: Optional[str],
        depth: str,
        language: str,
        history: Optional[list[dict]] = None,
        max_retries: int = 3,
    ) -> str:
        """
        Generate a Q&A response about the user's MBTI result.
        """
        await self.initialize()
        
        context = self._build_context(
            mbti_type=mbti_type,
            type_name=type_name,
            group=group,
            confidence_score=confidence_score,
            cognitive_stack=cognitive_stack,
            development_level=development_level,
            depth=depth,
            language=language,
        )
        
        conversation = []
        
        conversation.append({
            "role": "user",
            "parts": [f"{QA_SYSTEM_PROMPT}\n\n{context}"]
        })
        conversation.append({
            "role": "model",
            "parts": ["æ˜ç™½äº†ã€‚æˆ‘ä¼šç”¨æ¸©æš–ä¸“ä¸šçš„æ–¹å¼å¸®åŠ©ç”¨æˆ·ç†è§£ä»–ä»¬çš„MBTIç»“æœã€‚æˆ‘å‡†å¤‡å¥½å›ç­”é—®é¢˜äº†ã€‚"]
        })
        
        if history:
            for msg in history:
                conversation.append({
                    "role": msg.get("role", "user"),
                    "parts": [msg.get("content", "")]
                })
        
        conversation.append({
            "role": "user",
            "parts": [user_question]
        })
        
        last_error: Optional[Exception] = None
        
        for attempt in range(max_retries):
            try:
                logger.info("Generating Q&A response, attempt %d/%d", attempt + 1, max_retries)
                
                response = await self._model.generate_content_async(contents=conversation)
                
                if not response.candidates or not response.candidates[0].content.parts:
                    raise ValueError("No valid response from Gemini")
                
                response_text = response.text
                
                if not response_text:
                    raise ValueError("Empty response from Gemini")
                
                logger.info("Q&A response generated, length: %d", len(response_text))
                return response_text
                
            except google_exceptions.ResourceExhausted as e:
                logger.warning("Rate limit exceeded: %s", e)
                last_error = e
                import asyncio
                await asyncio.sleep(2 ** attempt)
                
            except google_exceptions.DeadlineExceeded as e:
                logger.warning("Request timeout: %s", e)
                last_error = e
                import asyncio
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error("Error in Q&A generation: %s", e, exc_info=True)
                last_error = e
                break
        
        logger.error("Q&A generation failed. Last error: %s", last_error)
        raise RuntimeError(f"AI service error: {last_error}")


# Singleton instances
ai_service = AIService()
qa_service = QAService()
